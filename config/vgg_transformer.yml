vocab_size: 233
vocab: models/definitions/vocab.yml
device: 'cuda'
weights: models/weights/transformer/base_transformer.pth
# weights: models/weights/transformer/vtp/2110270930/2110270945.pth

save_file:
    excel: result/tf/2110270945.xlsx
    text: result/tf/2110270945.txt

trainer:
    batch_size: 8
    display_cycle: 100
    valid_cycle: 1000
    iters: 100000
    weight_dir: models/weights/transformer/general/

dataset:    
    name: 'general'
    data_root: ./dataset/images/
    train_annotation: ./dataset/annotation/general/train.txt
    valid_annotation: ./dataset/annotation/general/val.txt
    test_annotation: ./dataset/annotation/vtp/test.txt
    expected_height: 64
    image_min_width: 64
    image_max_width: 4096

seq_modeling: transformer
transformer:  
    d_model: 256
    nhead: 8
    num_encoder: 6
    num_decoder: 6
    dim_feedforward: 2048
    max_seq: 1024
    pos_dropout: 0.1
    trans_dropout: 0.1

optimizer:
    max_lr: 0.0003
    pct_start: 0.1

dataloader:
    num_workers: 4
    pin_memory: True

aug:
    image_aug: True
    masked_language_model: True

backbone: vgg19
cnn_args:
    pretrained: False
    ss: [[2, 2],
         [2, 2],
         [2, 1],
         [2, 1],
         [2, 1]]
    ks: [[2, 2],
         [2, 2],
         [2, 1],
         [2, 1],
         [2, 1]
    hidden: 256,
    dropout: 0.1
